# -*- coding: utf-8 -*-
"""RandomForest_AishwaryaP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15-vdpFpFB7hkypm3PKW2-XkDLXFZj3dS

## Import the required libraries

---
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from datetime import datetime
import calendar

"""##Data Loading

---


"""

#Load data in  dataframe
train_df =  pd.read_csv('/content/drive/MyDrive/CS_235-Group16-Final_Project/NYCTaxiFare_TrainData.csv', nrows = 10000, parse_dates=["pickup_datetime"])
# list first few rows (datapoints)
train_df.head()

"""Check datatypes"""

train_df.dtypes

"""Training data description"""

train_df.describe()

sns.distplot(train_df['fare_amount'])
plt.title('Distribution of Fare Amount')

"""##Data Preprocessing

---

###Removal all the taxi trips that has negative fare amount.
"""

#Remove data which has the fare amount negative.
print('Old size: %d' % len(train_df))
train_df = train_df[train_df.fare_amount>=0]
print('New size: %d' % len(train_df))

"""###Removal taxi trips with missing data."""

#count of missing data with respect to attributes:
print(train_df.isnull().sum())

#Removing the records with missing data.
print('Old size: %d' % len(train_df))
train_df = train_df.dropna(how = 'any', axis = 'rows')
print('New size: %d' % len(train_df))

"""###Removal of taxi trips with passenger count less than zero and greater than six."""

print('Old size: %d' % len(train_df))
train_df = train_df[train_df.passenger_count<=6]
train_df = train_df[train_df.passenger_count>0]
print('New size: %d' % len(train_df))

passenger = train_df.groupby(['passenger_count']).count()
sns.barplot(passenger.index, passenger['key'], palette = "Set3")
plt.xlabel('Number of Passengers')
plt.ylabel('Count')
plt.title('Count of Passengers')
plt.show()

train_df.head()

def groupandplot(data,groupby_key,value,aggregate='mean'):
    agg_data=data.groupby([groupby_key])[value].agg(aggregate).reset_index().rename(columns={value:aggregate+'_'+value})
    sns.barplot(x=groupby_key,y=aggregate+'_'+value,data=agg_data).set_title(aggregate+'_'+value+" vs "+groupby_key)

groupandplot(train_df,'passenger_count','fare_amount')

"""###Removal of taxi trips having zero feature value."""

print('Old size: %d' % len(train_df))
train_df = train_df.loc[~(train_df == 0).any(axis=1)]
print('New size: %d' % len(train_df))

"""###Converting the pickup_datetime attribute of type Object to different primitive types using lambda functions."""

train_df['pickup_datetime']=pd.to_datetime(train_df['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')

train_df['pickup_date']= train_df['pickup_datetime'].dt.date
train_df['pickup_day']=train_df['pickup_datetime'].apply(lambda x:x.day)
train_df['pickup_hour']=train_df['pickup_datetime'].apply(lambda x:x.hour)
train_df['pickup_day_of_week']=train_df['pickup_datetime'].apply(lambda x:calendar.day_name[x.weekday()])
train_df['pickup_month']=train_df['pickup_datetime'].apply(lambda x:x.month)
train_df['pickup_year']=train_df['pickup_datetime'].apply(lambda x:x.year)

"""###Statistical Visualizations using Matplotlib"""

day_count = train_df.groupby(['pickup_day_of_week']).count().sort_values(by = 'key', ascending = False)
sns.barplot(day_count.index, day_count['key'], palette = "Set3")
plt.xlabel('Day of Week')
plt.ylabel('Count')
plt.title('Count of Taxi Rides per Day of Week')

day_count = train_df.groupby(['pickup_day']).count().sort_values(by = 'key', ascending = False)
sns.barplot(day_count.index, day_count['key'], palette = "Set3")
plt.xlabel('Day of month')
plt.ylabel('Count')
plt.title('Count of Taxi Rides per Day of Month')

day_fare = train_df.groupby(['pickup_day_of_week']).mean().sort_values(by = 'fare_amount', ascending = False)

sns.barplot(day_fare.index, day_fare.fare_amount, palette = "Set3")

plt.xlabel('Day of Week')
plt.ylabel('Average Fare Amount')
plt.title('Average Fare Amount per Day of Week')

day_sum = train_df.groupby(['pickup_day_of_week']).sum().sort_values(by = 'fare_amount', ascending = False)

sns.barplot(day_sum.index, day_sum.fare_amount, palette = "Set3")

plt.xlabel('Day of Week')
plt.ylabel('Count')
plt.title('Count of Total Rides Given During Each Day')

time_of_day = train_df.groupby(['pickup_hour']).mean()
plt.plot(time_of_day.index, time_of_day.fare_amount, color = 'r')
plt.xlabel('Hour')
plt.ylabel('Fare Price')
plt.title('Average Fare Price During Time of Day')
plt.show()

passenger_fare = train_df.groupby(['passenger_count']).mean()
sns.barplot(passenger_fare.index, passenger_fare['fare_amount'], palette = "Set3")
plt.xlabel('Number of Passengers')
plt.ylabel('Average Fare Price')
plt.title('Average Fare Price for Number of Passengers')
plt.show()

# Let us encode day of the week to numbers
def encodeDays(day_of_week):
    day_dict={'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6}
    return day_dict[day_of_week]

train_df['pickup_day_of_week']=train_df['pickup_day_of_week'].apply(lambda x:encodeDays(x))

train_df.dtypes

train_df = train_df.drop(['key','pickup_datetime','pickup_date'], axis=1)
train_df.head()

"""##Location data"""

# this function will also be used with the test set below
def select_within_boundingbox(df, BB):
    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \
           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \
           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \
           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])

train_df.head()

def distance(lat1, lat2, lon1,lon2):
    p = 0.017453292519943295 # Pi/180
    a = 0.5 - np.cos((lat2 - lat1) * p)/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) / 2
    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a))

train_df['trip_distance']=train_df.apply(lambda row:distance(row['pickup_latitude'],row['dropoff_latitude']
                                                             ,row['pickup_longitude'],row['dropoff_longitude']),axis=1)

sns.kdeplot(train_df['trip_distance'].values).set_title("Distribution of Trip Distance (log scale)")

a_plot=sns.lmplot(x='trip_distance', y='fare_amount',data=train_df)
a_plot.set(xlim=(0, 80))
a_plot.set(ylim=(0, 500))

plt.title('Distance of Ride Vs Taxi Fare')
# Set x-axis label
plt.xlabel('Distance of Ride')
# Set y-axis label
plt.ylabel('Taxi Fare')

train_df.head()

long_trips=train_df[train_df['trip_distance']>=50]

nyc_boroughs={
    'manhattan':{
        'min_lng':-74.0479,
        'min_lat':40.6829,
        'max_lng':-73.9067,
        'max_lat':40.8820
    },
    
    'queens':{
        'min_lng':-73.9630,
        'min_lat':40.5431,
        'max_lng':-73.7004,
        'max_lat':40.8007

    },

    'brooklyn':{
        'min_lng':-74.0421,
        'min_lat':40.5707,
        'max_lng':-73.8334,
        'max_lat':40.7395

    },

    'bronx':{
        'min_lng':-73.9339,
        'min_lat':40.7855,
        'max_lng':-73.7654,
        'max_lat':40.9176

    },

    'staten_island':{
        'min_lng':-74.2558,
        'min_lat':40.4960,
        'max_lng':-74.0522,
        'max_lat':40.6490
        
    }  
}

def getBorough(lat,lng):
    
    locs=nyc_boroughs.keys()
    for loc in locs:
        if lat>=nyc_boroughs[loc]['min_lat'] and lat<=nyc_boroughs[loc]['max_lat'] and lng>=nyc_boroughs[loc]['min_lng'] and lng<=nyc_boroughs[loc]['max_lng']:
            return loc
    return 'others'

train_df['pickup_borough']=train_df.apply(lambda row:getBorough(row['pickup_latitude'],row['pickup_longitude']),axis=1)
train_df['dropoff_borough']=train_df.apply(lambda row:getBorough(row['dropoff_latitude'],row['dropoff_longitude']),axis=1)

plt.figure(figsize=(8,5))
sns.countplot(y=train_df['pickup_borough'])
plt.title("Distribution of Pickup Boroughs")

plt.figure(figsize=(16,10))
plt.title("Distribution of Fare Amount Across Buroughs")
i=1
for key in nyc_boroughs.keys():
    plt.subplot(3,2,i)
    sns.kdeplot(np.log(train_df.loc[train_df['pickup_borough']==key,'fare_amount'].values),label='Pickup '+ key)
    sns.kdeplot(np.log(train_df.loc[train_df['dropoff_borough']==key,'fare_amount'].values),label='Dropoff'+ key).set_title("Fare Amount (log scale) for "+key)
    i=i+1

plt.figure(figsize=(24,15))
plt.title("Distribution of Trip Distances Across Buroughs")
i=1
for key in nyc_boroughs.keys():
    plt.subplot(3,2,i)
    sns.kdeplot(train_df.loc[train_df['pickup_borough']==key,'trip_distance'].values,label='Pickup '+ key)
    sns.kdeplot(train_df.loc[train_df['dropoff_borough']==key,'trip_distance'].values,label='Dropoff'+ key).set_title
    ("Trip Distance (log) for "+key)
    i=i+1

lower_manhattan_boundary={'min_lng': -74.0194,
                          'min_lat':40.6997,
                          'max_lng':-73.9716,
                          'max_lat':40.7427}

def isLowerManhattan(lat,lng):
    if lat>=lower_manhattan_boundary['min_lat'] and lat<=lower_manhattan_boundary['max_lat'] and lng>=lower_manhattan_boundary['min_lng'] and lng<=lower_manhattan_boundary['max_lng']:
        return 1
    else:
        return 0

train_df['is_pickup_lower_manhattan']=train_df.apply(lambda row:isLowerManhattan(row['pickup_latitude'],row['pickup_longitude']),axis=1)
train_df['is_dropoff_lower_manhattan']=train_df.apply(lambda row:isLowerManhattan(row['dropoff_latitude'],row['dropoff_longitude']),axis=1)

train_df.head()

nyc_airports={'JFK':{'min_lng':-73.8352,
     'min_lat':40.6195,
     'max_lng':-73.7401, 
     'max_lat':40.6659},
              
    'EWR':{'min_lng':-74.1925,
            'min_lat':40.6700, 
            'max_lng':-74.1531, 
            'max_lat':40.7081

        },
    'LaGuardia':{'min_lng':-73.8895, 
                  'min_lat':40.7664, 
                  'max_lng':-73.8550, 
                  'max_lat':40.7931
        
    }
    
}
def isAirport(latitude,longitude,airport_name='JFK'):
    
    if latitude>=nyc_airports[airport_name]['min_lat'] and latitude<=nyc_airports[airport_name]['max_lat'] and longitude>=nyc_airports[airport_name]['min_lng'] and longitude<=nyc_airports[airport_name]['max_lng']:
        return 1
    else:
        return 0

train_df['is_pickup_JFK']=train_df.apply(lambda row:isAirport(row['pickup_latitude'],row['pickup_longitude'],'JFK'),axis=1)
train_df['is_dropoff_JFK']=train_df.apply(lambda row:isAirport(row['dropoff_latitude'],row['dropoff_longitude'],'JFK'),axis=1)

train_df['is_pickup_EWR']=train_df.apply(lambda row:isAirport(row['pickup_latitude'],row['pickup_longitude'],'EWR'),axis=1)
train_df['is_dropoff_EWR']=train_df.apply(lambda row:isAirport(row['dropoff_latitude'],row['dropoff_longitude'],'EWR'),axis=1)

train_df['is_pickup_la_guardia']=train_df.apply(lambda row:isAirport(row['pickup_latitude'],row['pickup_longitude'],'LaGuardia'),axis=1)
train_df['is_dropoff_la_guardia']=train_df.apply(lambda row:isAirport(row['dropoff_latitude'],row['dropoff_longitude'],'LaGuardia'),axis=1)

mean=np.mean(train_df['pickup_hour'].value_counts())
dev=np.std(train_df['pickup_hour'].value_counts())
taxi_count=train_df['pickup_hour'].value_counts()
m1=mean-dev
m2=mean+dev

from sklearn.preprocessing import LabelEncoder

number = LabelEncoder()
train_df["pickup_borough"] = number.fit_transform(train_df["pickup_borough"].astype('str'))
train_df["dropoff_borough"] = number.fit_transform(train_df["dropoff_borough"].astype('str'))

"""##Feature Elimination using Spearman correlation"""

spearman_correlation = train_df.corr(method="spearman")

spearman_correlation['features'] = spearman_correlation.index
print(spearman_correlation['fare_amount'])

ax = spearman_correlation.plot.bar(x='features', y='fare_amount', rot=90)

print(train_df.columns)

train_df = train_df.drop(['is_pickup_EWR', 'pickup_day', 'pickup_day_of_week', 'pickup_month'],axis=1)

"""##Splitting dataset into Train and Test sets"""

X=train_df.drop(['fare_amount'],axis=1)
y=train_df['fare_amount']
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)

"""##Custom Implementation of Decision Tree Regressor

---


"""

# Importing basic libraries
import numpy as np
import pandas as pd

# Class for constructing decision tree node
class TreeNode():
    def __init__(self, 
                 idxFeature=None, 
                 variance_reduction=None, 
                 threshold_value=None, 
                 tree_left=None, 
                 tree_right=None,                 
                 leafNode_value=None):
        self.idxFeature = idxFeature        
        self.variance_reduction = variance_reduction
        self.threshold_value = threshold_value
        self.tree_left = tree_left
        self.tree_right = tree_right
        self.leafNode_value = leafNode_value

class Decision_Tree_Regressor():
    def __init__(self, 
                 min_samples_split=2, 
                 max_depth=2):
        self.root = None
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth        
    
    # This function helps determine leaf node value by computing mean. 
    def compute_leaf_value(self, Y):        
        mean_value = np.mean(Y)
        return mean_value  

    # This function helps split data based on threshold value.
    def split(self, 
              dataframe, 
              idxFeature, 
              threshold_value):        
        dataframe_tree_left = np.array([dr for dr in dataframe 
                                        if dr[idxFeature]<=threshold_value])
        dataframe_tree_right = np.array([dr for dr in dataframe 
                                         if dr[idxFeature]>threshold_value])
        
        return dataframe_tree_left, dataframe_tree_right
    
     # This function helps calculate variance reduction of a Node.
    def variance_reduction(self, 
                           parent_Node, 
                           leftChild_Node, 
                           rightChild_Node):
        nParentNode = len(parent_Node)
        nleft = len(leftChild_Node) / nParentNode
        nright = len(rightChild_Node) / nParentNode

        weighted_vreduc_left_right = (nleft * np.var(leftChild_Node) + 
                                   nright * np.var(rightChild_Node))
        vreduction_value = np.var(parent_Node) - weighted_vreduc_left_right

        return vreduction_value

    # This function recursively builds the tree on basis of best splitting criterion.    
    def construct_tree(self, 
                       dataframe, 
                       currentDepth=0):
        data_X = dataframe[:,:-1]
        data_Y = dataframe[:,-1]

        split_criterion_best = {}

        nSamples, nFeatures = np.shape(X)

        # split until stopping conditions are met
        if nSamples >= self.min_samples_split and currentDepth <= self.max_depth:
            # find the best split
            split_criterion_best = self.get_split_criterion_best(dataframe, nSamples, nFeatures)
            # check if information gain is positive
            if "variance_reduction" in split_criterion_best and split_criterion_best["variance_reduction"]>0:
                # recursively call left and right sub tree
                tree_left_subtree = self.construct_tree(split_criterion_best["dataframe_tree_left"], currentDepth+1)
                tree_right_subtree = self.construct_tree(split_criterion_best["dataframe_tree_right"], currentDepth+1)
                # return decision node
                return TreeNode(idxFeature = split_criterion_best["idxFeature"], 
                                variance_reduction = split_criterion_best["variance_reduction"],
                                threshold_value = split_criterion_best["threshold_value"], 
                                tree_left = tree_left_subtree, 
                                tree_right = tree_right_subtree)
        
        # compute leaf node
        leaf_value = self.compute_leaf_value(data_Y)
        # return leaf node
        return TreeNode(leafNode_value=leaf_value)
    
    # This function helps find the best splitting node.
    def get_split_criterion_best(self, 
                                 dataframe, 
                                 nSamples, 
                                 nFeatures):
              
        # Maintain a dictionary to store the best split
        split_criterion_best = {}

        # To keep track of max variance for a feature.
        max_variance_reduction = -float("inf")

        # loop over all the features
        for idxFeature in range(nFeatures):
            feature_values = dataframe[:, idxFeature]
            possible_threshold_values = np.unique(feature_values)
            # loop over all the feature values present in the data
            for threshold_value in possible_threshold_values:
                # get current split
                dataframe_tree_left, dataframe_tree_right = self.split(dataframe, idxFeature, threshold_value)
                # check if childs are not null
                if len(dataframe_tree_left)>0 and len(dataframe_tree_right)>0:
                    parent_y = dataframe[:, -1]
                    tree_left_y = dataframe_tree_left[:, -1]
                    tree_right_y = dataframe_tree_right[:, -1]
                    # compute information gain
                    cur_variance_reduction = self.variance_reduction(parent_y, tree_left_y, tree_right_y)
                    # update the best split only if max variance reduction is less than new value .i.e good split
                    if cur_variance_reduction > max_variance_reduction: 
                        # Update max variance reduction
                        max_variance_reduction = cur_variance_reduction     
                        # Update corresponding values.
                        split_criterion_best["variance_reduction"] = cur_variance_reduction
                        split_criterion_best["threshold_value"] = threshold_value
                        split_criterion_best["idxFeature"] = idxFeature            
                        split_criterion_best["dataframe_tree_left"] = dataframe_tree_left
                        split_criterion_best["dataframe_tree_right"] = dataframe_tree_right

        # return best split
        return split_criterion_best  

    # Function to train the decision tree.
    def fit(self, 
            X, 
            Y):
        dataframe = np.concatenate((X, Y), axis=1)
        self.root = self.construct_tree(dataframe)

    # Predict Function
    def predict(self, 
                X):
        preditions = [self.make_prediction(x, self.root) for x in X]
        return preditions  

    # Helper function for Predict funtion to determine the new data value.    
    def make_prediction(self, 
                        xNode, 
                        decisionTree):
              
        if decisionTree.leafNode_value!=None: 
          return decisionTree.leafNode_value

        feature_val = xNode[decisionTree.idxFeature]
        if feature_val <= decisionTree.threshold_value:
            return self.make_prediction(xNode, decisionTree.tree_left)
        else:
            return self.make_prediction(xNode, decisionTree.tree_right)

TreeRegressor = Decision_Tree_Regressor(min_samples_split = 3, max_depth = 3)

y_train = y_train.values.reshape(-1,1)

TreeRegressor.fit(X_train,y_train)

X_test = X_test.values

Y_pred = TreeRegressor.predict(X_test) 

TreeRegressor_r2 = r2_score(Y_pred, y_test)
print("R2 for Custom Decision Tree is ",TreeRegressor_r2)

TreeRegressor_RMSE = np.sqrt(mean_squared_error(Y_pred, y_test))
print("RMSE for Custom Decision Tree is ",TreeRegressor_RMSE)

"""##SKLEARN Implementation of Decision Tree Regressor

---


"""

from sklearn.tree import DecisionTreeRegressor

SkLearn_TreeRegressor = DecisionTreeRegressor(min_samples_split = 3, max_depth = 3)
SkLearn_TreeRegressor.fit(X_train, y_train)

SkLearn_Y_pred = SkLearn_TreeRegressor.predict(X_test) 

SkLearn_TreeRegressor_r2 = r2_score(SkLearn_Y_pred, y_test)
print("R2 for SKlearn Decision Tree is ", SkLearn_TreeRegressor_r2)

SkLearn_TreeRegressor_RMSE = np.sqrt(mean_squared_error(SkLearn_Y_pred, y_test))
print("RMSE for SKlearn Decision Tree is ", SkLearn_TreeRegressor_RMSE)

"""
##Custom Implementation of Random Forest Regressor

---

"""

#Helper functions for custom random forest regressor

# Bootstraping helps divide the data for trees construction
def bootstrap_sample(X, 
                     y):
    nSamples_data = X.shape[0] 
    indices = np.random.choice(nSamples_data,nSamples_data, replace=True)
    return X[indices], y[indices]
    
# This function helps determine leaf node value by computing mean.
def combined_average_label(y):
    mean_val = np.mean(y)
    return mean_val;

class Random_Forest_Regressor:
    def __init__(self, 
                 n_trees=10, 
                 min_samples_split=2, 
                 max_depth=100):
        self.n_trees = n_trees
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.trees = []
 
    # Fits data into model and trains our random forest model.
    def fit(self, 
            X, 
            y):
        # holds collection of trees.
        self.trees = []
        # Build n trees by splitting the data with replacement using bootstrapping.
        for _ in range(self.n_trees):
            decisionTree = Decision_Tree_Regressor(min_samples_split = self.min_samples_split,
                                                   max_depth = self.max_depth)
            X_samp, y_samp = bootstrap_sample(X, y)
            # Fit the bootstrapped data into the custom decision tree.
            decisionTree.fit(X_samp, y_samp)
            self.trees.append(decisionTree)

    # Predict funtion determines the new data value by taking the mean of all the results from n decision trees.
    def predict(self, 
                X):
        decisionTree_predictions = np.array([decisionTree.predict(X) for decisionTree in self.trees])
        decisionTree_predictions = np.swapaxes(decisionTree_predictions, 0, 1)
        y_predictions = [combined_average_label(decisiontree_pred) for decisiontree_pred in decisionTree_predictions]
        return np.array(y_predictions)

ForestRegressor = Random_Forest_Regressor(n_trees = 10, min_samples_split = 2, max_depth = 100)
X_train_values = X_train.values
ForestRegressor.fit(X_train_values, y_train)

Y_pred = ForestRegressor.predict(X_test) 

ForestRegressor_r2 = r2_score(Y_pred, y_test)
print("R2 for Custom Random Forest regression model is ", ForestRegressor_r2)

ForestRegressor_RMSE = np.sqrt(mean_squared_error(Y_pred, y_test))
print("RMSE for Custom Random Forest regression model is ", ForestRegressor_RMSE)

"""##SKLEARN Implementation of Random Forest Regressor

---


"""

from sklearn.ensemble import RandomForestRegressor

SkLearn_ForestRegressor =  RandomForestRegressor(n_estimators = 10, min_samples_split = 2, max_depth = 100)
SkLearn_ForestRegressor.fit(X_train_values, y_train.ravel())

SkLearn_Y_pred = SkLearn_ForestRegressor.predict(X_test) 

SkLearn_ForestRegressor_r2 = r2_score(SkLearn_Y_pred, y_test)
print("R2 for SKlearn Random Forest regression model is ", SkLearn_ForestRegressor_r2)

SkLearn_ForestRegressor_RMSE = np.sqrt(mean_squared_error(SkLearn_Y_pred, y_test))
print("RMSE for SKlearn Random Forest regression model is ", SkLearn_ForestRegressor_RMSE)